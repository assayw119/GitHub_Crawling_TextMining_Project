{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install requests\n",
    "! pip install beautifulsoup4\n",
    "! pip install lxml\n",
    "! pip install pandas\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = input(\"스타수 입력 : \")\n",
    "url = \"https://github.com/search?p=1&q=stars%3A%3E{}&type=Repositories\".format(stars)\n",
    "\n",
    "def crawling_func(url):\n",
    "    print(url)\n",
    "    try:\n",
    "        res = requests.get(url,headers=headers)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text,\"lxml\")\n",
    "        p = soup.find_all(\"a\",attrs={\"class\":\"v-align-middle\"}) \n",
    "    except:\n",
    "        time.sleep(1)\n",
    "        crawling_func(url) # 오류를 대비하기위해 재귀함수 호출\n",
    "        pass\n",
    "    finally:\n",
    "        return p\n",
    "    \n",
    "topic_ads = []\n",
    "pages = int(input(\"검색할 페이지 수(Ex : 10) : \"))\n",
    "print(\"{}개 이상의 star수를 가진 Repository의 상위 {}페이지를 크롤링합니다. \".format(stars,pages))\n",
    "print()\n",
    "for i in range(1,pages+1):# x 페이지까지 탐색\n",
    "    url = \"https://github.com/search?p={}&q=stars%3A%3E10000&type=Repositories\".format(i) # 페이지 formatting\n",
    "    time.sleep(15)  # 10초를 쉬어도 오류가 떴었음\n",
    "    for j in crawling_func(url):\n",
    "        topic_ads.append(j.get_text())\n",
    "        print(j.get_text())\n",
    "\n",
    "        \n",
    "topics_dic = {}\n",
    "topics_list = []\n",
    "for ad in topic_ads:\n",
    "    url_topic = \"https://github.com/\" + ad\n",
    "    res_topic = requests.get(url_topic,headers=headers)\n",
    "    res_topic.raise_for_status()\n",
    "    soup_topic = BeautifulSoup(res_topic.text,\"lxml\")\n",
    "    \n",
    "    topic = soup_topic.find(\"div\",attrs={\"class\":\"BorderGrid-cell\"}).find_all(\"a\",attrs={\"class\":\"topic-tag topic-tag-link\"})\n",
    "    project_topics=[i.get_text().replace(\"\\n\",\"\").replace(\"\\t\",\"\").strip() for i in topic]\n",
    "    \n",
    "    # 스타 수(북마크 수, 인기도)\n",
    "    star_num = soup_topic.find(\"ul\",attrs={\"class\":\"pagehead-actions flex-shrink-0 d-none d-md-inline\"}).find(\"a\",attrs={\"class\":\"social-count js-social-count\"}).get_text()\n",
    "    star_num = star_num.replace('\\t', '').replace('\\n', '').strip()\n",
    "\n",
    "    topics_list.append([ad,project_topics,star_num])\n",
    "    \n",
    "    for t in project_topics:\n",
    "        if t in topics_dic:\n",
    "            topics_dic[t] += 1\n",
    "        else:\n",
    "            topics_dic[t] = 1\n",
    "\n",
    "# value로 내림차순 정렬\n",
    "topics_dic= sorted(topics_dic.items(), key=lambda x: x[1], reverse=True)\n",
    "df_topic2 = pd.DataFrame(topics_list,columns=['project_name','topic_keyword','star_number'])\n",
    "# 엑셀로 저장하기\n",
    "\n",
    "df_topic2.to_excel('Topics_stars{}_project_keyword.xlsx'.format(stars),index=False)\n",
    "print()\n",
    "print(\"star 수가 {} 개 이상인 프로젝트 상위 {}페이지에대한 Data가 저장되었습니다.\")\n",
    "print(\"종료 .\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
